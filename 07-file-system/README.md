# 파일시스템

리눅스에서는 저장 장치 안의 데이터에 접근 할 때 일반적으로 직접 저장 장치에 접근하지 않고 편의를 위해 파일시스템을 통해 접근한다.

컴퓨터 시스템에 파일시스템이 있는 것은 당연한 일처럼 여겨지지만, 파일시스템이 없는 세상을 상상해보며 **파일시스템의 필요성**에 대해 살펴보자.

저장 장치의 기능은 단순하게 말하면 '저장 장치 안에 지정된 주소에 대해 특정 사이즈의 데이터를 읽거나 쓰는 것'이다. 만약 파일시스템이 없다면 저장 장치에 쓰고 읽기 위해서는 다음과 같은 정보를 별도로 관리해야 한다.

![모든 데이터에 대해 보관한 주소, 사이즈를 관리할 필요가 있음](/images/07-file-system/7-2.png)  

이러한 복잡한 처리를 피하고자, 어디에 어느 정도의 데이터가 있는지, 어디가 빈 영역인지를 관리하는 방식이 파일시스템이다.

### 파일 시스템의 역할
파일 시스템은 사용자에 의미가 있는 하나의 데이터를 **이름, 위치, 사이즈** 등의 **보조 정보**를 추가하여 **파일**이라는 단위로 관리한다. 어느 장소에 어떤 파일을 배피할 지 등의 데이터 구조는 사양으로 미리 정하며, 커널 내부에서 파일시스템을 다루기 위한 처리가 사양을 바탕으로 데이터를 다루게 된다.

![단순한 파일시스템의 예](/images/07-file-system/7-3.png)  

사용자가(정확히는 사용자의 프로세스가) 파일 읽기 시스템 콜을 사용하여 파일 이름과 파일상의 오프셋 및 사이즈를 지정하면 파일시스템을 다루는 처리가 해당하는 데이터를 찾아서 사용자에게 전달한다.

![파일 이름, 파일상의 오프셋, 사이즈를 지정하면 해당하는 데이터를 읽을 수 있음](/images/07-file-system/7-4.png)  

### 리눅스의 파일 시스템
파일을 카테고리 별로 정리할 수 있도록 리눅스의 파일시스템에는 디렉터리(directory)라고 부르는 파일을 보관하는 특수한 파일이 존재한다. 이를 위해 리눅스의 파일시스템은 **트리 구조**로 되어 있다.

![트리 구조의 파일시스템](/images/07-file-system/7-5.png) 

### 파일시스템 콜(call)
리눅스가 다루는 파일시스템은 1개가 아니라 'ext4', 'XFS', 'Btrfs' 등 여러 개의 파일시스템을 다룰 수 있다. 각 파일시스템은 저장 장치의 데이터 구조 및 그것을 처리하기 위한 프로그램이 다르며, 다룰 수 있는 파일의 사이즈, 파일시스템의 사이즈, 개개의 처리(파일 작성, 삭제, 읽기, 쓰기 등)의 속도 등도 다르다.

하지만 **어떤 파일시스템이라도 사용자가 다음과 같은 시스템 콜을 호풀한다면 통일된 인터페이스로 접근이 가능하다**.

* 파일의 작성, 삭제: create(), unlink()
* 파일을 열기, 닫기: open(), close()
* 데이터 읽기: read()
* 데이터 쓰기: write()
* 파일 이동: lseek()
* 위에 언급한 것 이외의 파일 시스템에 의존적인 특수한 처리: ioctl()

위의 시스템 콜이 호출되면 다음과 같은 순서로 파일의 데이터가 읽어진다.

1. 커널 내의 모든 파일시스템 공통 처리가 동작하고 대상 파일의 파일시스템을 판별한다.
2. 각 파일시스템을 처리하는 프로세스를 호출하여 시스템 콜에 대응되는 처리를 한다.
3. 데이터의 읽기를 하는 경우 디바이스 드라이버에 처리를 의뢰한다.
4. 디바이스 드라이버가 데이터를 읽어들인다.

![파일시스템에 관계없이 통일된 인터페이스로 접근 가능함](/images/07-file-system/7-6.png) 

# 데이터와 메타데이터
파일시스템에는 데이터(data)와 메타데이터(metadata)라는 두 종류의 데이터가 있다.
* 데이터: 사용자가 작성한 문서나 사진, 동영상 프로그램 등의 내용
* 메타데이터: 파일의 보조 정보
  * 파일의 이름이
  * 저장 장치 내에 위치 사이즈
  * 종류: 데이터를 보관하는 일반 파일 or 디렉토리 or 다른 종류의 파일인지 판별하는 정보
  * 시간 정보: 작성한 시간, 최후에 접근한 시간, 최후에 내용이 변경된 시간
  * 권한 정보: 사용자 접근 권한

  >(참고) `df` 명령어로 얻은 파일시스템의 스토리지 사용량은 파일 시스템에 작성한 모든 파일의 합계 사이즈만이 아니라 **메타데이터의 사이즈도 더해지므로** 주의가 필요하다.

![파일시스템에 관계없이 통일된 인터페이스로 접근 가능함](/images/07-file-system/code1.png) 

# 용량 제한

파일시스템의 용량을 용도별로 사용할 수 있게 제한하는 **쿼터(quota)**라는 기능이 있다.

### 용량 제한(quota)이 필요한 이유
시스템을 여러 가지 용도로 사용하는 경우 특정 용도가 파일시스템의 용량을 무제한으로 사용할 수 있다면 다른 용도로 사용할 용량이 부족하게 되는 일이 발생한다.

특히 root 권한으로 동작하는 프로세스가 이용하는 시스템 관리 처리를 위한 용량이 부족하게 되면 **시스템 전체가 정상적으로 동작할 수 없게 된다**.

![파일시스템의 용량이 부족하게 되면 시스템이 정상적으로 동작하지 않음](/images/07-file-system/7-7.png) 

### Quota의 종류
quota에는 다음과 같은 종류가 있다.

* 사용자 쿼터: 파일의 소유자인 사용자 별로 용량을 제한한다. 예를 들어 특정 사용자 때문에 /home 용량이 부족한 사태를 방지한다. ext4와 XFS에서 사용 가능하다.
* 디렉토리 쿼터(혹은 프로젝트 쿼터): 특정 디렉토리 별로 용량을 제한하는 것이다. 예를 들어 프로젝트 멤버가 공유하는 디렉토리에 용량 제한을 걸어둔다. ext4와 XFS에서 사용 가능하다.
* 서브 볼륨 쿼터: 파일시스템 내의 서브 볼륨이라는 단위별 용량을 제한하는 것이다. 디렉토리 쿼터와 사용 방법이 유사하며, Btrfs에서 사용 가능하다.

![쿼터](/images/07-file-system/7-8.png) 

# 파일시스템이 깨진 경우
시스템을 운용하다보면 종종 파일시스템의 내용이 깨지는 경우가 발생한다. 전형적인 예로 파일시스템의 데이터를 스토리지에 쓰고 있는 도중에 시스템의 전원이 강제적으로 끊어졌을 때 같은 경우에 발생한다.

### 파일시스템이 깨진 상태
구체적으로 파일시스템이 깨진 경우란 어떤 상태인지 살펴보자. 아래의 예시는 **어떤 디렉토리를 다른 디렉토리 아래로 이동하는 경우이다**.

![디렉토리의 이동](/images/07-file-system/7-9.png) 

이것의 흐름을 그림으로 나타내면 [그림 7-10]과 같다.

![디렉토리의 이동 처리의 흐름](/images/07-file-system/7-10.png) 

이러한 일련의 처리는 **하나라도 누락되면 안되므로 '아토믹(atomic)'한 처리**라고 부른다.

저장 장치의 읽고 쓰기는 한 번에 한 가지씩만 처리되므로, 첫번째 쓰기(foo 파일의 데이터 업데이트)가 끝난 뒤에 두 번째 쓰기(root 데이터 업데이트)를 하기 전에 처리가 중단되면 [그림 7-11]과 같이 파일시스템이 깨진 상태가 된다.

![파일시스템이 깨짐](/images/07-file-system/7-11.png) 

일단 이런 일이 발생하면 빠르든 늦든 언제가는 파일시스템이 감지하게 된다. 마운트 시 발견할 경우에는 파일시스템의 마운트가 불가능하게 되고, 파일시스템에 접근 중에 발생한 경우에는 읽기 전용 모드로 다시 마운트(remount)하거나 최악의 경우 시스템에 패닉이 발생한다.

파일시스템이 깨지는 것을 방지하기 위한 기술은 여러 가지가 있다. 그 중에서 널리 사용되고 있는 것은 '저널링'과 'Copy on Write'라는 두 가지 방식이다. **ext4와 XFS는 저널링**으로, **Btrfs는 Copy on Write**로 각각 파일 시스템이 깨지는 것을 방지하고 있다.

### 저널링: ext4, XFS
저널링에서는 파일시스템 안에 저널 영역이라는 특수한 영역을 준비한다. 저널 영역은 사용자가 인식할 수 없는 메타데이터이다.

![저널링 방식에 의한 업데이트 처리](/images/07-file-system/7-12.png) 

파일시스템을 업데이트할 때에는 다음과 같은 순서로 진행된다.

1) 업데이트에 필요한 아토믹한 처리의 목록을 일단 저널 영역에 작성한다. 이 목록을 **저널로그**라고 부른다.
2) 저널 영역의 내용을 바탕으로 실제로 파일시스템의 내용을 업데이트한다.

#### 저널로그 작성 중 재부팅하는 경우
저널로그 업데이트 중에 강제로 전원을 끊어보면 [그림 7-12]과 같이 단순히 저널 영역의 데이터를 지워버릴 뿐, 실제 데이터는 처리하기 전과 같다.

![저널링을 이용해 깨짐을 방지함(1)](/images/07-file-system/7-13.png) 

#### 실제 데이터 작성 중 재부팅하는 경우
실제 데이터를 업데이트 하는 중에 강제로 전원이 끊어지는 경우가 발생한다면 [그림 7-14]와 같이 저널로그를 처음부터 다시 수행하면 파일시스템의 처리가 정상적으로 완료된다.

![저널링을 이용해 깨짐을 방지함(2)](/images/07-file-system/7-14.png) 

### Copy on Write: Btrfs
Copy on Write를 이용해 파일시스템이 깨지는 것을 방지하는 방식을 이해하기 위해서는 우선 파일 시스템에 데이터를 넣는 방법에 대해 이해해야 한다.

**ext4와 XFS 등의 예전부터 있는 파일시스템**은 일단 파일을 작성하면 그 파일의 배치 주소는 원칙적으로 바뀌지 않는다. 파일의 내용을 업데이트할 때마다 저장 장치 상의 같은 주소에 새로운 데이터를 써넣는다.

![Copy on Write 방식이 아닌 경우 파일의 업데이트](/images/07-file-system/7-15.png) 

이와 다르게 **Btrfs 등의 Copy on Write 유형의 파일시스템**은 일단 파일을 작성하더라고 업데이트할 때 다른 주소에 데이터를 쓰고 링크를 교체 연결한다. 

![Copy on Write 방식을 이용한 단순한 업데이트 처리](/images/07-file-system/7-16.png) 

[그림 7-16]은 단지 하나의 파일을 업데이트 하는 경우였지만 **아토믹으로 처리해야 할 여러 개의 처리를 실행할 경우에도 업데이트되는 데이터를 다른 주소에 전부 쓴 뒤에 링크를 고쳐쓰는 방식**으로 동작한다.

![Copy on Write 방식을 이용한 복잡한 업데이트 처리](/images/07-file-system/7-17.png) 

[그림 7-18]과 같이 강제 전원 단절이 발생하더라도 재부팅 후에 작성된 데이터를 삭제하면 문제 없다.

![Copy on Write 방식(3)](/images/07-file-system/7-18.png) 

### 파일시스템의 깨짐에 대한 대책
앞에서 설명한 기능을 사용하여 최근에는 파일시스템이 깨지는 경우가 줄어들고 있다. 그러나 파일시스템의 버그가 원인인 손상을 매우 적지만 여전히 발생할 수 있다.

일반적으로는 파일시스템을 **정기적으로 백업**하여 파일시스템이 깨진 경우에 마지막에 백업한 시점의 상태로 복원하는 것이 대책이다.

그러나 **정기적으로 백업이 불가능**하면 각 파일시스템에 준비된 복구용 명령어를 이용한다. 복구용 명령어는 파일시스템마다 다르다.

그러나 어느 파일시스템이라도 공통적으로 존재하는 것이 `fsck`라는 명령어(ext4라면 `fsck.ext4`, XFS라면 `xfs_repair`, Btrfs라면 `btrfs check`)이다. 이 명령어를 사용하면 파일시스템을 깨지지 않은 상태로 롤백할 수 있다. 그러나 **`fsck`는 다음과 같은 이유로 별로 추천하지 않는다**.

1. 깨지지 않았음을 확인하거나 복구하기 위해 파일시스템 전체를 조사하기 때문에 소요 시간이 파일시스템 사용량에 따라 증가한다.
2. 복구에 오랜 시간을 들여도 결국 실패하고 끝나는 경우도 많다.
3. 사용자가 원하는 상태로 복원한다는 보장이 없다. fsck는 어디까지나 데이터가 깨진 파일시스템을 무리해서라도 마운트 하려는 명령어에 지나지 않는다. **처리하면서 깨진 데이터는 내용과 관계없이 삭제한다**.

![fsck의 동작)](/images/07-file-system/7-19.png) 

# 파일의 종류
파일에는 사용자의 데이터를 보관하는 `일반 파일`과 파일을 보관하는 `디렉토리`가 있다. 리눅스는 그 외에도 `디바이스 파일`이라는 종류가 있다.

**리눅스는 스스로 동작하고 있는 하드웨어 상의 장치를 거의 모두 파일로서 표현하고 있다(네트워크는 어댑터는 예외로 장치에 대응되는 파일이 없다).** 그렇기 때문에 리눅스에서는 장치를 파일과 동등하게 `open()`과 `read()`, `write()` 등의 시스템 콜을 사용한다. 장치 고유의 복잡한 조작에는 `ioctl()` 시스템 콜을 사용한다. 참고로 디바이스 파일에 접근할 수 있는 것은 일반적으로 root만 가능하다.

리눅스는 파일로써 접근하는 디바이스를 **캐릭터 장치**와 **블록 장치**라는 두가지 종류로 분류한다. 각 디바이스 파일은 `/dev` 아래에 존재한다. 디바이스 파일의 메타데이터에 보관되어 있는 다음의 정보에 따라 각각의 장치를 식별한다.

* 파일의 종류(캐릭터 or 블록)
* 장치의 Major number
* 장치의 Minor number

Major number와 Minor number의 차이를 자세히 알 필요는 없다.

`/dev` 이하의 파일을 살펴보자.

![`/dev` 이하의 디바이스 파일)](/images/07-file-system/code2.png) 

`ls -l`의 실행 결과로 최초의 문자다 `c`라면 캐릭터 장치, `b`라면 블록장치이다. 다섯 번째 필드가 Major number, 여섯 번째 필드가 Minor number이다.

위에서 `/dev/tty`는 캐릭터 장치, `/dev/sda`는 블록 장치인 것을 알 수 있다.

### 캐릭터 장치
캐릭터 장치는 **읽기와 쓰기는 가능하지만 탐색(seek)이 되지 않는 특성**이 있다. 대표적인 캐릭터 장치는 다음과 같은 것들이 있다.

* 터미널: 터미널에도 여러 가지가 있기 때문에 정확하게 정의하기는 어렵지만 여기서는 bash 등의 쉘을 통해서 명령어를 실행하기 위해 문자열만으로 이루어진 흑백 화면 혹은 윈도우라고 생각하면 된다.
* 키보드
* 마우스

예를들어 터미널의 디바이스 파일은 다음과 같이 다룬다.
* write() 시스템 콜: 터미널에 데이터를 출력한다.
* read() 시스템 콜: 터미널에 데이터를 입력한다.

실제 터미널을 조작해보자. 일단 현재의 프로세스에 대응하는 터미널과 그 터미널에 대응하는 디바이스 파일을 찾아보자. **각 프로세스에 연결되어 있는 터미널에 대응하는 장치는 `ps ax` 명령어의 두 번째 필드**로 알아낼 수 있다.

![프로세스에 연결되어있는 터미널 확인](/images/07-file-system/code3.png) 

위의 결과에서 bash에 연결되어 있는 터미널에 대응하는 장치는 `/dev/pts/9`이다. 이 파일에 적당한 문자열을 입력해보자.

![장치에 문자열 데이터 쓰기](/images/07-file-system/code4.png) 

터미널 디바이스에 'hello'라는 문자열을 쓰는(정확히는 디바이스 파일에 write() 시스템 콜을 요청)것으로 퍼미널에는 이 문자열을 출력되었다. 이것은 단순하게 `echo hello` 명령어를 실행한 것과 같은 결과이다. echo는 표준 출력에 'hello'를 쓰고 있고, 리눅스에 의해 표준 출력이 현재의 터미널에 연결되어 있기 때문이다.

계속해서 시스템에 존재하는 다른 터미널에서도 조작해보자. 일단 새로운 터미널을 하나 더 가동한 뒤 `ps ax`를 실행해보자.

![새로운 터미널에서 `ps ax` 명령 수행](/images/07-file-system/code5.png) 

두 번째 터미널에 대응하는 디바이스 이름은 `/dev/pts/10`이다. 이 파일에 문자열을 써보도록 하자.

![장치에 문자열 데이터 쓰기](/images/07-file-system/code6.png) 

두 번째 터미널을 살펴보면 이 단말에는 키보드 입력을 아무것도 하지 않았ㅈ만 앞선 터미널의 디바이스 파일에 썼던 문자렬이 출력되는 것을 알 수 있다.

![문자열 출력](/images/07-file-system/code7.png) 

실제로는 애플리케이션이 터미널의 디바이스 파일을 직접 조작하는 일은 많지 않다. 그러나 보통 익숙한 bash의 조작이 최종적으로 디바이스 파일의 조작으로 변환되어 있음을 이해하고 있다면 문제 없다.

### 블록 장치
블록 장치는 단순히 파일의 읽고 쓰기 이외에 **랜덤 접근이 가능**하다. 대표적인 블록 장치는 **HDD**나 **SSD** 등의 저장장치가 있다. 블록 장치에 데이터를 읽고 쓰는 것으로 일반적인 파일처럼 스토리지의 특정 장소에 있는 데이터에 접근할 수 있다.

블록 장치는 **일반적으로 직접 접근하지 않고** 거기에 **파일시스템을 작성해서 그것을 마운트함으로써 파일시스템을 경유**해서 사용한다. 

블록장치를 **직접 다루는 것**은 다음과 같은 경우이다.

* 파티션 테이블의 업데이트(parted 명령어 등을 사용)
* 블록 장치 레벨의 데이터 백업 & 복구(dd 명령어 등을 사용)
* 파일시스템의 작성(각 파일시스템의 mkfs 명령어를 사용)
* 파일시스템의 마운트(mount 명령어를 사용)
* fsck


# 여러 가지 파일시스템
지금까지 ext4, XFS, Btrfs라는 파일시스템을 알아보았다. 이러한 파일시스템은 저장 장치상에 존재하는 것이었다. 그러나 리눅스에는 이외에도 여러 가지 종류의 파일시스템이 있다.

### 메모리 기반 파일시스템
저장 장치 대신 메모리에 작성하는 **'tmpfs'**라는 파일시스템이 있다. 

* 파일시스템에 보존된 데이터는 전원을 꺼버리면 사라진다.
* 저장 장치의 접근이 전혀 발생하지 않기 때문에 [그림 7-20]과 같이 고속으로 사용할 수 있다.

![tmpfs 파일시스템](/images/07-file-system/7-20.png) 

tmpfs는 재부팅 후에 남아있을 필요가 없는 `/tmp`나 `/var/run`에 사용하는 경우가 많다.

![tmpfs 용도](/images/07-file-system/code8.png) 

tmpfs는 마운트 할 때 작성한다. 이때 'size' 마운트 옵션으로 최대 사이즈를 사용하도록 하고 있다(처음부터 이 최대 용량의 메모리를 확보하지는 않고, 파일시스템 내의 각 영역이 처음 접근할 때, 페이지 단위로 메모리를 확보하는 방식이므로 문제가 없다).

`free` 명령어 출력결과에 있는 'shared' 필드 값이 tmpfs에 의해 실제로 사용된 메모리의 양을 표시한다.

![`free` 명령어 결과](/images/07-file-system/code9.png) 

### 네트워크 파일시스템
지금까지 설명한 파일시스템은 로컬 시스템에 있는 데이터를 보여주는 것이었지만 **네트워크를 통해 연결된 원격 호스트에 있는 파일에 접근**하는 네트워크 파일시스템이라는 것도 있다.

![네트워크 파일시스템](/images/07-file-system/7-21.png) 

네트워크 파일시스템에도 여러 가지 종류가 있지만 크게 나누어 보면 다음과 같다.

* windows 호스트 상의 파일에 접근할 때 사용하는 파일시스템: cifs
* linux 등 UNIX 계열의 OS를 사용하는 호스트 상의 파일에 접근할 때 사용하는 파일시스템: nfs

### 가상 파일시스템
커널 안에 여러 가지 정보를 얻기 위해 또는 커널의 동작을 변경하기 위해서 다양한 파일시스템이 존재한다.

#### procfs
시스템에 존재하는 프로세스에 대한 정보를 얻기 위해 존재한다.일반적으로 `/proc` 이하에 마운트 되며, `/proc/pid/`이하의 파일에 접근함으로써 각 프로세스의 정보를 얻을 수 있다.

![procfs](/images/07-file-system/code10.png) 

수 많은 파일이 있지만 몇 가지만 소개하면 다음과 같다.

* /proc/pid/maps: 프로세스의 메모리 맵
* /proc/pid/cmdline: 프로세스의 명령어 라인 파라미터
* /proc/pid/stat: 프로세스의 상태, 지금까지 사용한 CPU 시간, 우선도, 사용 메모리 양 등

(참고) procfs에는 프소세스 정보 외에도 다음과 같은 정보도 있다.

* /proc/cpuinfo: 시스템이 탑재한 CPU에 대한 정보
* /proc/diskstat: 시스템이 탑재한 저장 장치에 대한 정보
* /proc/meminfo: 시스템의 메모리에 대한 정보
* /proc/sys 이하의 파일: 커널의 각종 튜닝 파라미터, `sysctl`와 `/etc/sysctl.conf`로 변경하는 파라미터와 1:1 대응

지금까지 나왔던 ps, sar, top, free 등의 OS가 제공하는 각종 정보를 표시하는 명령어는 procfs로부터 정보를 얻고 있다.

#### sysfs
리눅스에 procfs가 도입되고 시간이 지남에 따라 커널의 프로세스 정보 이외의 잡다한 정보가 procfs에 제한 없이 들어가게 되었다. **procfs를 마구잡이로 사용하는 것을 막기 위해, 이러한 정보를 배치하는 장소를 어느 정도 정하기 위해** 만든 것이 `sysfs`이다. sysfs는 보통 `/sys` 이하에 마운트 된다.

sysfs에는 예를 들어 다음과 같은 파일이 있다
* /sys/devices 이하의 파일: 시스템에 탑재된 디바이스에 대한 정보
* /sys/fs: 시스템에 존재하는 각종 파일시스템에 대한 정보

#### cgroupfs
하나의 프로세스 혹은 여러 개의 프로세스로 만들어진 그룹에 대해 여러 가지 **리소스 사용량의 제한을 가하는** 'cgoup'이라는 기능이 있다. cgroup은 `cgroupfs`라는 파일시스템을 통해 다루며, root만 다룰 수 있다. cgroupfs는 일반적으로 `/sys/fs/cgroup` 이하에 마운트 된다. 

cgroup로 제한할 수 있는 리소스로는 여러 가지가 있다.

* CPU: 그룹이 CPU 전체 리소스 중 50% 등, 일정한 비율 이상은 사용할 수 없도록 하는 등 `/sys/fs/cgroup/cpu` 이하의 파일을 읽고 쓰는 것으로 제어할 수 있다.
* 메모리: 그룹이 물리 메모리 중 특정량 밖에 사용하지 못하게 하는 등 `/sys/fs/cgroupd/memory` 이하의 파일을 읽고 쓰는 것으로 제어할 수 있다.

cgroup은 docker 등의 컨테이너 관리 소프트웨어나 virt-manager 등의 가상 시스템 관리 소프트웨어 등에 각각의 컨테이너나 가상 시스템의 리소스를 제한하기 위해 사용할 수 있다. **특히 하나의 시스템상에 여러 개의 컨테이너나 가상 시스템이 공존하는 서버 시스템에 사용**되고 있다.

# Btrfs
ext4나 XFS는 작은 차이는 있지만 기본적으로 리눅스의 원조인 UNIX가 만들어졌을 때부터 있는 파일의 작성, 삭제, 읽고 쓰기 등의 단순한 기능만 제공하고 있다.

그러나 최근에는 이러한 것 이상으로 풍부한 기능을 제공하는 파일시스템이 나왔는데, 그 중 대표인 **Btrfs**가 제공하는 기능을 소개한다.

### 1. 멀티 볼륨
ext4나 XFS는 하나의 파티션에 대응하여 하나의 파일시스템을 만든다. **Btrfs**는 하나 뿐만이 아니라 여러 개의 저장 장치/파티션으로부터 거대한 **스토리지 풀**을 만들고 거기에 마운트 가능한 **서브 볼륨**이라는 영역을 작성한다. **스토리지 풀은 LVM으로 구현된 볼륨 그룹**, **서브 볼륨은 LVM으로 구현된 논리 볼륨과 파일시스템을 더한 것**과 비슷하다.

따라서 Btrfs는 종래의 파일시스템의 한 종류라고 생각하기 보다는 **파일시스템 + LVM과 같은 볼륨 매니저**라고 생각하면 이해에 도움이 된다.

![Btrfs는 종래의 파일시스템 + LVM의 기능을 갖춤](/images/07-file-system/7-22.png) 

이미 만들어진 Btrfs 파일시스템에 저장 장치의 추가, 삭제, 교환도 가능하며, 이러한 작업을 할때 용량 변화에 따른 파일시스템의 크기를 조정 처리하는 것은 불필요하다.

![Btrfs는 저장 장치의 추가, 삭제, 교환도 가능](/images/07-file-system/7-23.png) 

이러한 처리는 **전부 파일시스템의 마운트 중에도 운영을 멈추지 않고 할 수 있다**.

### 2. 스냅샷
Btrfs는 **서브 볼륨 단위로 스냅샷**을 만들 수 있다. 스냅샷은 데이터를 전부 복사하는 것이 아니라 데이터를 참조하는 메타데이터의 작성 혹은 스냅샷 내의 더티 페이지의 라이트 백을 하는 것만으로 처리할 수 있기 때문에 일반적인 복사보다 훨씬 빠르게 이루어진다. 원래의 서브 볼륨과 스냅샷은 데이터를 공유하기 때문에 공간 적인 낭비도 적다.

일반 복사와 Btrfs에 의한 서브 볼륨의 스냅샷을 비교하면 다음과 같다.

[그림 7-22]에서 일반 복사시 메타데이터를 새로 만든 뒤에 데이터를 전부 복사하는 것을 알 수 있다.

![일반 복사](/images/07-file-system/7-24.png) 

반면에 [그림 7-23]에서 스냅샷의 경우 트리의 root 노드만을 새로 만들어 거기에 다음 레벨의 노드를 링크 거는 것만으로도 처리가 끝난다. 즉, 데이터의 복사는 발생하지 않기 때문에 복사보다 훨씬 고속으로 작성이 가능하다.

![스냅샷](/images/07-file-system/7-25.png) 

### 3. RAID
Btrfs에는 파일시스템 레벨에 RAID(Redundant Array of Inexpensive Disk, 데이터 저장의 성능 및 안정성 확보를 위해 복수의 디스크를 구성 [자세한 내용](https://m.blog.naver.com/PostView.nhn?blogId=hostinggodo&logNo=220641452948&proxyReferer=https:%2F%2Fwww.google.com%2F)) 작성을 포함하고 있다. 지원되는 것은 RAID 0, 1, 10, 5, 6 거기에 dup(같은 데이터를 같은 저장 장치에 이중화 하나의 장치용)이다. 어느 RAID 레벨로 설정되는지의 단위는 서브 볼륨이 아닌 Btrfs 파일시스템 전체이다.

#### RAID가 필요한 이유
RAID가 없는 구성의 경우를 살펴보자. [그림 7-26]과 같이 RAID가 없는 경우 sda가 망가지면 데이터를 전부 잃게 된다.

![RAID가 없는 경우 sda가 망가지면 데이터를 전부 잃게 됨](/images/07-file-system/7-26.png) 

반면에 파일시스템 RAID1 로 구성했다면, 모든 데이터는 2개의 스토리지(이경우 sda, sdb)에 저장되기 때문에 sda가 망가지더라도 A의 데이터는 sdb에 남아 보존된다.

![RAID1 구성이라면 sda가 망가져도 데이터를 잃어버리지 않음](/images/07-file-system/7-27.png) 

#### 데이터의 파손, 검출, 복구
Btrfs는 스토리지 내의 일부 데이터가 파괴된 경우 이것을 검출해 몇 가지의 RAID 구성으로 복구가 가능하다. 만약 이러한 기능을 가지지 않은 파일시스템이라면 [그림 7-28]과 같이 데이터가 파손된 것을 모르는 채로 운영을 계속하게 되는 리스크가 있다.

![데이터가 깨진 것을 검출하지 못하면 모르는 채로 운영을 계속하게 되는 리스크가 있음](/images/07-file-system/7-28.png) 

반면에 Btrfs는 데이터, 메타데이터 모두 일정의 데이터 크기마다 **체크섬(checksum)**을 가지고 있으므로 데이터의 파손을 검출할 수 있다. [그림 7-29]와 같이 데이터(혹은 메타데이터)를 읽는 도중 (1)체크섬 에러를 검출하게 되면 (2)그 데이터를 버리고 3)읽기를 요청한 프로그램에 I/O에러를 알린다.

![데이터의 피손을 체크섬으로 검출](/images/07-file-system/7-29.png) 

* RAID 1, 10,, 5, 6, dup의 구성이면 다른 체크선이 일치하는 정확한 데이터를 기준으로 파괴된 데이터를 복구한다.
* RAID 5, 6의 경우에도 패리티를 사용해서 같은 것을 할 수 있다.

[그림 7-30]은 RAID1로 구성된 경우 복구의 흐름을 나타낸다. 이 경우 읽기를 요청한 곳은 데이터가 일시적으로 깨졌음을 인식하지 못하고 지나가게 된다.

![RAID 1, 10, 5, 6, dup의 경우 데이터의 파손을 복구할 수 있음](/images/07-file-system/7-30.png) 